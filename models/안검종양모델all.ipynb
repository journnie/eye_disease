{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 반려견 안구질환 진단 예측 프로젝트 - 두부는 단단"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "data = {\n",
    "    \"결막염\": [\"유\", \"무\"],\n",
    "    \"궤양성각막질환\": [\"상\", \"하\", \"무\"],\n",
    "    \"백내장\": [\"초기\", \"비성숙\", \"성숙\", \"무\"],\n",
    "    \"비궤양성각막질환\": [\"상\", \"하\", \"무\"],\n",
    "    \"색소침착성각막염\": [\"유\", \"무\"],\n",
    "    \"안검내반증\": [\"유\", \"무\"],\n",
    "    \"안검염\": [\"유\", \"무\"],\n",
    "    \"안검종양\": [\"유\", \"무\"],\n",
    "    \"유루증\": [\"유\", \"무\"],\n",
    "    \"핵경화\": [\"유\", \"무\"]\n",
    "}\n",
    "lesion_names = {\n",
    "    '결막염': 'conjunctivitis',\n",
    "    '궤양성각막질환': 'corneal_ulcer',\n",
    "    '백내장': 'cataract',\n",
    "    '비궤양성각막질환': 'non_ulcerative_keratitis',\n",
    "    '색소침착성각막염': 'pigmentary_keratitis',\n",
    "    '안검내반증': 'entropion',\n",
    "    '안검염': 'blepharitis',\n",
    "    '안검종양': 'eyelid_tumor',\n",
    "    '유루증': 'uveitis',\n",
    "    '핵경화': 'nuclear_sclerosis'\n",
    "}\n",
    "\n",
    "# dataframe\n",
    "def create_dataframe_from_json(directory_path):\n",
    "    data = []\n",
    "    for file_path in glob.glob(os.path.join(directory_path, \"*.json\")):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "            # Extract required fields\n",
    "            image_meta = json_data.get('images', {}).get('meta', {})\n",
    "            label_info = json_data.get('label', {})\n",
    "            data.append({\n",
    "                # 데이터프레임 생성을 위한 정보 추출\n",
    "                'breed': json_data[\"images\"][\"meta\"][\"breed\"],\n",
    "                'age': json_data[\"images\"][\"meta\"][\"age\"],\n",
    "                'gender' : json_data[\"images\"][\"meta\"][\"gender\"],\n",
    "                'eye_position' : json_data[\"images\"][\"meta\"][\"eye_position\"],\n",
    "                'lesions' : json_data[\"label\"][\"label_disease_nm\"],\n",
    "                'label_disease_lv_1' : json_data[\"label\"][\"label_disease_lv_1\"],\n",
    "                'label_disease_lv_2' : json_data[\"label\"][\"label_disease_lv_2\"],\n",
    "                'label_disease_lv_3' : json_data[\"label\"][\"label_disease_lv_3\"],\n",
    "                'img_path' : os.path.join(directory_path, json_data[\"label\"][\"label_filename\"])\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 증상 유무\n",
    "def map_and_check_disease_presence(row):\n",
    "    # Define a function to map values to 0 or 1\n",
    "    def map_to_binary(value):\n",
    "        return 0 if value == '무' else 1\n",
    "    \n",
    "    # Apply the function to the specified columns and check if any value is 1\n",
    "    for column in ['label_disease_lv_1', 'label_disease_lv_2', 'label_disease_lv_3']:\n",
    "        if map_to_binary(row[column]) == 1:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# 확장자 명 없는 파일경로 수정\n",
    "\n",
    "def check_and_correct_image_paths(df, path=0):\n",
    "    \"\"\"\n",
    "    Checks if the images paths exist in the 'img_path' column of the DataFrame.\n",
    "    If an image path does not exist, it appends the corresponding extension (.png or .jpg) to the image path in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the 'img_path' column.\n",
    "        path (int): If set to 0, '.png' will be appended to the image path if it does not exist, \n",
    "                    otherwise '.jpg' will be appended. Default is 0.\n",
    "    \"\"\"\n",
    "    for idx, img_path in enumerate(df['img_path']):\n",
    "        if not os.path.exists(img_path):\n",
    "            if path == 0:\n",
    "                df.at[idx, 'img_path'] += '.png'\n",
    "            else:\n",
    "                df.at[idx, 'img_path'] += '.jpg'\n",
    "\n",
    "# Example usage:\n",
    "# check_and_correct_image_paths(df, path=0)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_image_data(df, target_size=(100, 100)):\n",
    "    \"\"\"\n",
    "    Preprocesses image data from the DataFrame by loading images, resizing them, and converting them to numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the 'img_path' column.\n",
    "        target_size (tuple): Target size for resizing images. Default is (100, 100).\n",
    "\n",
    "    Returns:\n",
    "        X_aug (ndarray): Numpy array containing preprocessed image data.\n",
    "        Y_aug (ndarray): Numpy array containing labels.\n",
    "        test_datagen (ImageDataGenerator): ImageDataGenerator object fitted to the preprocessed image data.\n",
    "    \"\"\"\n",
    "    X_aug = []\n",
    "    for img_path in df['img_path']:\n",
    "        img = Image.open(img_path).resize(target_size)\n",
    "        img_array = np.asarray(img)\n",
    "        X_aug.append(img_array)\n",
    "    X_aug = np.stack(X_aug, axis=0)\n",
    "    Y_aug = np.array(df.iloc[:, -1:])\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen.fit(X_aug)\n",
    "    \n",
    "    return X_aug, Y_aug\n",
    "\n",
    "# Example usage:\n",
    "# X_aug, Y_aug, test_datagen = prepare_image_data(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, kernel_size = (3,3), input_shape = [100, 100, 3], activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2), padding = 'same'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2), padding = 'same'))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2), padding = 'same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(7, activation='sigmoid'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "    model.compile(loss = 'sparse_categorical_crossentropy',\n",
    "                 optimizer = optimizer,\n",
    "                  metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, Y_train, EPOCHS):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='auto')\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(X_train, Y_train, validation_split=0.2, batch_size = 64, epochs = EPOCHS,\n",
    "                        callbacks = [reduce_lr, early_stop])\n",
    "    print(\"h2\")\n",
    "    return history\n",
    "\n",
    "def plot_model_training_curve(history):\n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=['Model Accuracy', 'Model Loss'])\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=history.history['accuracy'], name='train_acc'), row=1, col=1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=history.history['val_accuracy'], name='val_acc'), row=1, col=1)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=history.history['loss'], name='train_loss'), row=1, col=2)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            y=history.history['val_loss'], name='val_loss'), row=1, col=2)\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 주어진 데이터로 모든 폴더에서 데이터프레임 생성\n",
    "all_dfs = []\n",
    "for disease, symptoms in data.items():\n",
    "    for symptom in symptoms:\n",
    "        for dataset_type in [\"1.Training\", \"2.Validation\"]:\n",
    "            directory_path = f\"D:\\\\반려동물 안구질환 데이터\\\\{dataset_type}\\\\{disease}\\\\{symptom}\"\n",
    "            print(directory_path)\n",
    "            df = create_dataframe_from_json(directory_path)\n",
    "            all_dfs.append(df)\n",
    "            \n",
    "            # Apply the combined function to create 'path' column\n",
    "            df['path'] = df.apply(map_and_check_disease_presence, axis=1)\n",
    "            # 확장자 명 없는 파일경로 수정\n",
    "            check_and_correct_image_paths(df, path=0)\n",
    "            \n",
    "            X_aug, Y_aug = prepare_image_data(df)\n",
    "            \n",
    "            epochs = 50\n",
    "            model=create_model()\n",
    "            print('------------------------------------------------------------------------')\n",
    "            history = train_model(model, X_aug, Y_aug, EPOCHS=epochs)\n",
    "            if dataset_type == \"1.Training\":\n",
    "                plot_model_training_curve(history)\n",
    "            else:\n",
    "                model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "                loss, accuracy = model.evaluate(X_aug, Y_aug, verbose=0)\n",
    "                print(f'Score: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
    "                model_file='안검종양모델'+'.h5'\n",
    "                model.save(model_file)\n",
    "                print(f'Test Loss: {loss}')\n",
    "                print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "# 생성된 데이터프레임들을 하나로 합치기\n",
    "merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "# 생성된 데이터프레임 출력\n",
    "print(merged_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"결막염\": [\"유\", \"무\"],\n",
    "    \"궤양성각막질환\": [\"상\", \"하\", \"무\"],\n",
    "    \"백내장\": [\"초기\", \"비성숙\", \"성숙\", \"무\"],\n",
    "    \"비궤양성각막질환\": [\"상\", \"하\", \"무\"],\n",
    "    \"색소침착성각막염\": [\"유\", \"무\"],\n",
    "    \"안검내반증\": [\"유\", \"무\"],\n",
    "    \"안검염\": [\"유\", \"무\"],\n",
    "    \"안검종양\": [\"유\", \"무\"],\n",
    "    \"유루증\": [\"유\", \"무\"],\n",
    "    \"핵경화\": [\"유\", \"무\"]\n",
    "}\n",
    "lesion_names = {\n",
    "    '결막염': 'conjunctivitis',\n",
    "    '궤양성각막질환': 'corneal_ulcer',\n",
    "    '백내장': 'cataract',\n",
    "    '비궤양성각막질환': 'non_ulcerative_keratitis',\n",
    "    '색소침착성각막염': 'pigmentary_keratitis',\n",
    "    '안검내반증': 'entropion',\n",
    "    '안검염': 'blepharitis',\n",
    "    '안검종양': 'eyelid_tumor',\n",
    "    '유루증': 'uveitis',\n",
    "    '핵경화': 'nuclear_sclerosis'\n",
    "}\n",
    "\n",
    "# dataframe\n",
    "def create_dataframe_from_json(directory_path):\n",
    "    data = []\n",
    "    for file_path in glob.glob(os.path.join(directory_path, \"*.json\")):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "            # Extract required fields\n",
    "            image_meta = json_data.get('images', {}).get('meta', {})\n",
    "            label_info = json_data.get('label', {})\n",
    "            data.append({\n",
    "                # 데이터프레임 생성을 위한 정보 추출\n",
    "                'breed': json_data[\"images\"][\"meta\"][\"breed\"],\n",
    "                'age': json_data[\"images\"][\"meta\"][\"age\"],\n",
    "                'gender' : json_data[\"images\"][\"meta\"][\"gender\"],\n",
    "                'eye_position' : json_data[\"images\"][\"meta\"][\"eye_position\"],\n",
    "                'lesions' : json_data[\"label\"][\"label_disease_nm\"],\n",
    "                'label_disease_lv_1' : json_data[\"label\"][\"label_disease_lv_1\"],\n",
    "                'label_disease_lv_2' : json_data[\"label\"][\"label_disease_lv_2\"],\n",
    "                'label_disease_lv_3' : json_data[\"label\"][\"label_disease_lv_3\"],\n",
    "                'img_path' : os.path.join(directory_path, json_data[\"label\"][\"label_filename\"])\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 증상 유무\n",
    "def map_and_check_disease_presence(row):\n",
    "    # Define a function to map values to 0 or 1\n",
    "    def map_to_binary(value):\n",
    "        return 0 if value == '무' else 1\n",
    "    \n",
    "    # Apply the function to the specified columns and check if any value is 1\n",
    "    for column in ['label_disease_lv_1', 'label_disease_lv_2', 'label_disease_lv_3']:\n",
    "        if map_to_binary(row[column]) == 1:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# 확장자 명 없는 파일경로 수정\n",
    "\n",
    "def check_and_correct_image_paths(df, path=0):\n",
    "    \"\"\"\n",
    "    Checks if the images paths exist in the 'img_path' column of the DataFrame.\n",
    "    If an image path does not exist, it appends the corresponding extension (.png or .jpg) to the image path in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the 'img_path' column.\n",
    "        path (int): If set to 0, '.png' will be appended to the image path if it does not exist, \n",
    "                    otherwise '.jpg' will be appended. Default is 0.\n",
    "    \"\"\"\n",
    "    for idx, img_path in enumerate(df['img_path']):\n",
    "        if not os.path.exists(img_path):\n",
    "            if path == 0:\n",
    "                df.at[idx, 'img_path'] += '.png'\n",
    "            else:\n",
    "                df.at[idx, 'img_path'] += '.jpg'\n",
    "\n",
    "# Example usage:\n",
    "# check_and_correct_image_paths(df, path=0)\n",
    "\n",
    "\n",
    "\n",
    "def prepare_image_data(df, target_size=(100, 100)):\n",
    "    \"\"\"\n",
    "    Preprocesses image data from the DataFrame by loading images, resizing them, and converting them to numpy arrays.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the 'img_path' column.\n",
    "        target_size (tuple): Target size for resizing images. Default is (100, 100).\n",
    "\n",
    "    Returns:\n",
    "        X_aug (ndarray): Numpy array containing preprocessed image data.\n",
    "        Y_aug (ndarray): Numpy array containing labels.\n",
    "        test_datagen (ImageDataGenerator): ImageDataGenerator object fitted to the preprocessed image data.\n",
    "    \"\"\"\n",
    "    X_aug = []\n",
    "    for img_path in df['img_path']:\n",
    "        img = Image.open(img_path).resize(target_size) # default 100, 100\n",
    "        img_array = np.asarray(img)\n",
    "        X_aug.append(img_array)\n",
    "    X_aug = np.stack(X_aug, axis=0)\n",
    "    Y_aug = np.array(df.iloc[:, -1:])\n",
    "    \n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen.fit(X_aug)\n",
    "    \n",
    "    return X_aug, Y_aug\n",
    "\n",
    "# Example usage:\n",
    "# X_aug, Y_aug, test_datagen = prepare_image_data(df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, kernel_size = (3,3), input_shape = [100, 100, 3], activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2)))\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2), padding = 'same'))\n",
    "\n",
    "    model.add(Conv2D(64, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2), padding = 'same'))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size = (3,3), activation = 'relu', padding = 'same'))\n",
    "    model.add(MaxPool2D(pool_size = (2,2), padding = 'same'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "\n",
    "    model.compile(loss = 'binary_crossentropy',\n",
    "                 optimizer = optimizer,\n",
    "                  metrics = ['binary_accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def train_model(model, X_train, Y_train, EPOCHS):\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='auto')\n",
    "    \n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, mode='auto')\n",
    "    print(\"Training model...\")\n",
    "    history = model.fit(X_train, Y_train, validation_split=0.2, batch_size = 64, epochs = EPOCHS,\n",
    "                        callbacks = [reduce_lr, early_stop])\n",
    "    print(\"h2\")\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 주어진 데이터로 모든 폴더에서 데이터프레임 생성\n",
    "\n",
    "for disease, symptoms in data.items():\n",
    "    for dataset_type in [\"1.Training\", \"2.Validation\"]:\n",
    "        all_dfs = []\n",
    "        for symptom in symptoms:\n",
    "            directory_path = f\"D:\\\\반려동물 안구질환 데이터\\\\{dataset_type}\\\\{disease}\\\\{symptom}\"\n",
    "            print(directory_path)\n",
    "            df = create_dataframe_from_json(directory_path)\n",
    "            all_dfs.append(df)\n",
    "            \n",
    "        # Apply the combined function to create 'path' column\n",
    "        df['path'] = df.apply(map_and_check_disease_presence, axis=1)\n",
    "        # 확장자 명 없는 파일경로 수정\n",
    "        check_and_correct_image_paths(df, path=0)\n",
    "        \n",
    "        X_aug, Y_aug = prepare_image_data(df)\n",
    "        \n",
    "        \n",
    "        if dataset_type == \"1.Training\":\n",
    "            epochs = 50\n",
    "            model=create_model()\n",
    "            history = train_model(model, X_aug, Y_aug, EPOCHS=epochs)\n",
    "            model_file=f'{lesion_names[disease]}'+'.h5'\n",
    "            model.save(model_file)\n",
    "        elif dataset_type == \"2.Validation\":\n",
    "            loss, accuracy = model.evaluate(X_aug, Y_aug, verbose=0)\n",
    "            print(disease + ' Validation')\n",
    "            print(f'Test Loss: {loss}')\n",
    "            print(f'Test Accuracy: {accuracy}')\n",
    "\n",
    "\n",
    "# # 생성된 데이터프레임들을 하나로 합치기\n",
    "# merged_df = pd.concat(all_dfs, ignore_index=True)\n",
    "# \n",
    "# # 생성된 데이터프레임 출력\n",
    "# print(merged_df)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "dfs = [pd.DataFrame,'bbb']\n",
    "a, b =dfs\n",
    "a"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GoogleNet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결막염\n",
      "D:\\반려동물 안구질환 데이터\\1.Training\\결막염\\유\n",
      "D:\\반려동물 안구질환 데이터\\1.Training\\결막염\\무\n",
      "D:\\반려동물 안구질환 데이터\\2.Validation\\결막염\\유\n",
      "D:\\반려동물 안구질환 데이터\\2.Validation\\결막염\\무\n",
      "결막염 model\n",
      "Found 19198 validated image filenames belonging to 2 classes.\n",
      "Found 2403 validated image filenames belonging to 2 classes.\n",
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\aepython\\eye_lesions\\venv\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m600/600\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 326ms/step - accuracy: 0.9503 - loss: 0.1431\n",
      "Epoch 1: val_loss improved from inf to 2.89248, saving model to conjunctivitis.keras\n",
      "\u001B[1m600/600\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m226s\u001B[0m 343ms/step - accuracy: 0.9503 - loss: 0.1430 - val_accuracy: 0.9309 - val_loss: 2.8925\n",
      "Epoch 2/30\n",
      "\n",
      "Epoch 2: val_loss improved from 2.89248 to 0.00000, saving model to conjunctivitis.keras\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asiae\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m600/600\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m3s\u001B[0m 5ms/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 3/30\n",
      "\u001B[1m600/600\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 323ms/step - accuracy: 0.9891 - loss: 0.0375\n",
      "Epoch 3: val_loss did not improve from 0.00000\n",
      "\u001B[1m600/600\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m201s\u001B[0m 334ms/step - accuracy: 0.9891 - loss: 0.0375 - val_accuracy: 0.8906 - val_loss: 0.5368\n",
      "Epoch 4/30\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.00000\n",
      "\u001B[1m600/600\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 75us/step - accuracy: 0.0000e+00 - loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_loss: 0.0000e+00\n",
      "Epoch 5/30\n",
      "\u001B[1m 88/600\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m2:49\u001B[0m 331ms/step - accuracy: 0.9939 - loss: 0.0356"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPool2D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"결막염\": [\"유\", \"무\"],\n",
    "    \"궤양성각막질환\": [\"상\", \"하\", \"무\"],\n",
    "    \"백내장\": [\"초기\", \"비성숙\", \"성숙\", \"무\"],\n",
    "    \"비궤양성각막질환\": [\"상\", \"하\", \"무\"],\n",
    "    \"색소침착성각막염\": [\"유\", \"무\"],\n",
    "    \"안검내반증\": [\"유\", \"무\"],\n",
    "    \"안검염\": [\"유\", \"무\"],\n",
    "    \"안검종양\": [\"유\", \"무\"],\n",
    "    \"유루증\": [\"유\", \"무\"],\n",
    "    \"핵경화\": [\"유\", \"무\"]\n",
    "}\n",
    "lesion_names = {\n",
    "    '결막염': 'conjunctivitis',\n",
    "    '궤양성각막질환': 'corneal_ulcer',\n",
    "    '백내장': 'cataract',\n",
    "    '비궤양성각막질환': 'non_ulcerative_keratitis',\n",
    "    '색소침착성각막염': 'pigmentary_keratitis',\n",
    "    '안검내반증': 'entropion',\n",
    "    '안검염': 'blepharitis',\n",
    "    '안검종양': 'eyelid_tumor',\n",
    "    '유루증': 'uveitis',\n",
    "    '핵경화': 'nuclear_sclerosis'\n",
    "}\n",
    "\n",
    "def create_dataframes(disease):\n",
    "    all_dfs = [] # all_dfs[0]: train_df, all_dfs[1]: valid_df\n",
    "    for dataset_type in [\"1.Training\", \"2.Validation\"]:\n",
    "        df_sets = []\n",
    "        for symptom in data[disease]:\n",
    "            directory_path = f\"D:\\\\반려동물 안구질환 데이터\\\\{dataset_type}\\\\{disease}\\\\{symptom}\"\n",
    "            print(directory_path)\n",
    "            df = create_dataframe_from_json(directory_path)\n",
    "            df_sets.append(df)\n",
    "        concatenated_df = pd.concat(df_sets)\n",
    "        concatenated_df.reset_index(drop=True, inplace=True)\n",
    "        all_dfs.append(concatenated_df)\n",
    "            \n",
    "    # disease lv로 증상 유무 판단 -> 누락 이미지 경로 채우기\n",
    "    for df in all_dfs:       \n",
    "        df['path'] = df.apply(map_and_check_disease_presence, axis=1)\n",
    "        # 확장자 명 없는 파일경로 수정\n",
    "        check_and_correct_image_paths(df, path=0)\n",
    "        \n",
    "    train_df, valid_df = all_dfs\n",
    "    return train_df, valid_df\n",
    "\n",
    "# dataframe\n",
    "def create_dataframe_from_json(directory_path):\n",
    "    data = []\n",
    "    for file_path in glob.glob(os.path.join(directory_path, \"*.json\")):\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            json_data = json.load(file)\n",
    "            # Extract required fields\n",
    "            image_meta = json_data.get('images', {}).get('meta', {})\n",
    "            label_info = json_data.get('label', {})\n",
    "            data.append({\n",
    "                # 데이터프레임 생성을 위한 정보 추출\n",
    "                'breed': json_data[\"images\"][\"meta\"][\"breed\"],\n",
    "                'age': json_data[\"images\"][\"meta\"][\"age\"],\n",
    "                'gender' : json_data[\"images\"][\"meta\"][\"gender\"],\n",
    "                'eye_position' : json_data[\"images\"][\"meta\"][\"eye_position\"],\n",
    "                'lesions' : json_data[\"label\"][\"label_disease_nm\"],\n",
    "                'label_disease_lv_1' : json_data[\"label\"][\"label_disease_lv_1\"],\n",
    "                'label_disease_lv_2' : json_data[\"label\"][\"label_disease_lv_2\"],\n",
    "                'label_disease_lv_3' : json_data[\"label\"][\"label_disease_lv_3\"],\n",
    "                'img_path' : os.path.join(directory_path, json_data[\"label\"][\"label_filename\"])\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 증상 유무\n",
    "def map_and_check_disease_presence(row):\n",
    "    # Define a function to map values to 0 or 1\n",
    "    def map_to_binary(value):\n",
    "        return 0 if value == '무' else 1\n",
    "    \n",
    "    # Apply the function to the specified columns and check if any value is 1\n",
    "    for column in ['label_disease_lv_1', 'label_disease_lv_2', 'label_disease_lv_3']:\n",
    "        if map_to_binary(row[column]) == 1:\n",
    "            return '1'\n",
    "    return '0'\n",
    "\n",
    "# 확장자 명 없는 파일경로 수정\n",
    "\n",
    "def check_and_correct_image_paths(df, path=0):\n",
    "    \"\"\"\n",
    "    Checks if the images paths exist in the 'img_path' column of the DataFrame.\n",
    "    If an image path does not exist, it appends the corresponding extension (.png or .jpg) to the image path in the DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (DataFrame): The DataFrame containing the 'img_path' column.\n",
    "        path (int): If set to 0, '.png' will be appended to the image path if it does not exist, \n",
    "                    otherwise '.jpg' will be appended. Default is 0.\n",
    "    \"\"\"\n",
    "    for idx, img_path in enumerate(df['img_path']):\n",
    "        if not os.path.exists(img_path):\n",
    "            if path == 0:\n",
    "                df.at[idx, 'img_path'] += '.png'\n",
    "            else:\n",
    "                df.at[idx, 'img_path'] += '.jpg'\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "# check_and_correct_image_paths(df, path=0)\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "def create_data_generators(train_df, valid_df):\n",
    "    \"\"\"\n",
    "    Create data generators for training and validation dataframes.\n",
    "\n",
    "    Parameters:\n",
    "        train_df (DataFrame): DataFrame containing training data.\n",
    "        valid_df (DataFrame): DataFrame containing validation data.\n",
    "\n",
    "    Returns:\n",
    "        train_generator (DirectoryIterator): Data generator for training data.\n",
    "        val_generator (DirectoryIterator): Data generator for validation data.\n",
    "    \"\"\"\n",
    "    # 데이터 전처리\n",
    "    datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col='img_path',\n",
    "        y_col='path',\n",
    "        target_size=(100, 100),  # InceptionV3 모델의 입력 크기\n",
    "        batch_size=32,\n",
    "        class_mode='binary'  # 이진 클래스 분류\n",
    "    )\n",
    "\n",
    "    val_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        x_col='img_path',\n",
    "        y_col='path',\n",
    "        target_size=(100, 100),\n",
    "        batch_size=32,\n",
    "        class_mode='binary'\n",
    "    )\n",
    "\n",
    "    return train_generator, val_generator\n",
    "\n",
    "# Usage example:\n",
    "# train_generator, val_generator = create_data_generators(train_df, valid_df)\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "def conduct_googlenet():\n",
    "    # 구글넷 모델 불러오기\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\n",
    "    \n",
    "    # 모델 구성\n",
    "    model = Sequential()\n",
    "    model.add(base_model)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(1024, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))  # 클래스 수에 맞게 출력층 설정\n",
    "    \n",
    "    # 기존의 컴파일 및 학습 코드 추가\n",
    "    \n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer=Adam(),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "        \n",
    "\n",
    "def train_model_with_callbacks(model, train_generator, val_generator):\n",
    "    \"\"\"\n",
    "    Train the model with callbacks for model checkpointing and early stopping.\n",
    "\n",
    "    Parameters:\n",
    "        model (tf.keras.Model): The model to train.\n",
    "        train_generator (DirectoryIterator): Data generator for training data.\n",
    "        val_generator (DirectoryIterator): Data generator for validation data.\n",
    "        epochs (int): Number of epochs for training. Default is 30.\n",
    "        checkpoint_path (str): Path to save the best model checkpoint. Default is \"best_model.keras\".\n",
    "        patience (int): Number of epochs with no improvement after which training will be stopped. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "        history (tf.keras.callbacks.History): Object containing training history.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # ModelCheckpoint callback: Save the best model checkpoint based on validation loss\n",
    "    checkpoint = ModelCheckpoint(f'{lesion_names[disease]}.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # EarlyStopping callback: Stop training if validation loss does not improve for 'patience' epochs\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=len(train_generator),\n",
    "        epochs=30,\n",
    "        validation_data=val_generator,\n",
    "        validation_steps=len(val_generator),\n",
    "        callbacks=[checkpoint, early_stop]  # Add callbacks\n",
    "    )\n",
    "\n",
    "    return history\n",
    "\n",
    "# Usage example:\n",
    "# history = train_model_with_callbacks(model, train_generator, val_generator)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 주어진 데이터로 모든 폴더에서 데이터프레임 생성\n",
    "\n",
    "for disease in data.keys():\n",
    "    print(disease)\n",
    "    train_df, valid_df = create_dataframes(disease)\n",
    "    print(disease + ' model')\n",
    "    train_generator, val_generator = create_data_generators(train_df, valid_df)\n",
    "    model = conduct_googlenet()\n",
    "    history = train_model_with_callbacks(model, train_generator, val_generator)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-02T11:54:55.770704Z"
    }
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_df['path_str'] = train_df['path'].astype(str)\n",
    "valid_df['path_str'] = valid_df['path'].astype(str)\n",
    "\n",
    "# 데이터 전처리\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='img_path',\n",
    "    y_col='path_str',\n",
    "    target_size=(100, 100),  # InceptionV3 모델의 입력 크기\n",
    "    batch_size=32,\n",
    "    class_mode='binary'  # 이진 클래스 분류\n",
    ")\n",
    "\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "    dataframe=valid_df,\n",
    "    x_col='img_path',\n",
    "    y_col='path_str',\n",
    "    target_size=(100, 100),\n",
    "    batch_size=32,\n",
    "    class_mode='binary'\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "train_generator[0][1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# 구글넷 모델 불러오기\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "\n",
    "\n",
    "# 구글넷 모델 불러오기\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(100, 100, 3))\n",
    "\n",
    "# 모델 구성\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # 클래스 수에 맞게 출력층 설정\n",
    "\n",
    "# 기존의 컴파일 및 학습 코드 추가\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# ModelCheckpoint 콜백: 검증 손실이 낮아질 때 모델 저장\n",
    "checkpoint = ModelCheckpoint(\"best_model.keras\", monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "# EarlyStopping 콜백: 검증 손실이 5 에포크 동안 개선되지 않으면 학습 종료\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, verbose=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_df) // 32,\n",
    "    epochs=30,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=len(valid_df) // 32,\n",
    "    callbacks=[checkpoint, early_stop]  # 콜백 함수 추가\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.save('GoogleNet_eye.h5')"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image\n",
    "\n",
    "def predict_image(model_path, image_path, labels):\n",
    "    # 모델 불러오기\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "    # 이미지 불러오기 및 전처리\n",
    "    img = image.load_img(image_path, target_size=(100, 100))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.\n",
    "\n",
    "    # 예측 수행\n",
    "    predictions = model.predict(img_array)\n",
    "\n",
    "    # 예측 결과에서 가장 높은 확률을 가진 클래스 인덱스 찾기\n",
    "    predicted_class_index = np.argmax(predictions)\n",
    "    # 해당 클래스의 라벨\n",
    "    predicted_label = labels[predicted_class_index]\n",
    "    # 해당 클래스의 확률\n",
    "    predicted_prob = predictions[0][predicted_class_index]\n",
    "\n",
    "    return predicted_label, predicted_prob\n",
    "\n",
    "# 예측된 클래스의 라벨과 해당 라벨에 대한 확률을 출력하는 함수\n",
    "def print_prediction_result(predicted_label, predicted_prob):\n",
    "    print(\"Predicted label:\", predicted_label)\n",
    "    print(\"Predicted probability:\", predicted_prob)\n",
    "\n",
    "# 예시 이미지 경로\n",
    "image_paths = ['C:\\aepython\\eye_lesions\\models\\결막염 테스트\\dog.jpg','C:\\aepython\\eye_lesions\\models\\결막염 테스트\\dog2.jpg',\n",
    "               'C:\\aepython\\eye_lesions\\models\\결막염 테스트\\dog3.jpg','C:\\aepython\\eye_lesions\\models\\결막염 테스트\\dog5.jpg','C:\\aepython\\eye_lesions\\models\\결막염 테스트\\dog6.jpg','C:\\aepython\\eye_lesions\\models\\결막염 테스트\\dog7.jpg']\n",
    "\n",
    "# 모델 경로\n",
    "model_path = 'GoogleNet.h5'\n",
    "\n",
    "# 라벨 설정 (이진 분류일 경우)\n",
    "labels = ['0', '1']\n",
    "\n",
    "# 이미지 예측 수행\n",
    "for image_path in image_paths:\n",
    "    predicted_label, predicted_prob = predict_image(model_path, image_path, labels)\n",
    "    # 결과 출력\n",
    "    print_prediction_result(predicted_label, predicted_prob)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "model.summary()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# resnet"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "def create_generators(preprocess_func=None):\n",
    "    #rescale changes pixels from 1-255 integers to 0-1 floats suitable for neural nets\n",
    "    rescale = 1./255\n",
    "    if preprocess_func is not None:\n",
    "        #https://stackoverflow.com/questions/48677128/what-is-the-right-way-to-preprocess-images-in-keras-while-fine-tuning-pre-traine\n",
    "        #no need to rescale if using Keras in-built ResNet50 preprocess_func: https://github.com/keras-team/keras-applications/blob/master/keras_applications/imagenet_utils.py#L157\n",
    "        rescale = None\n",
    "\n",
    "    train_datagen=ImageDataGenerator(\n",
    "        rescale = rescale\n",
    "    )\n",
    "    #Keras has this two-part process of defining generators. \n",
    "    #First the generic properties above, then the actual generators with filenames and all.\n",
    "    train_generator=train_datagen.flow_from_dataframe(\n",
    "        dataframe=train_df,\n",
    "        x_col=\"img_path\", #the name of column containing image filename in dataframe\n",
    "        y_col=\"path\", #the y-col in dataframe\n",
    "        batch_size=batch_size, \n",
    "        # shuffle=False,\n",
    "        class_mode=\"binary\", #categorical if multiple. then y_col can be list or tuple also \n",
    "        #classes=lbls, #list of ouput classes. if not provided, inferred from data\n",
    "        target_size=(100,100),\n",
    "        subset='training') #the subset of data from the ImageDataGenerator definition above. The validation_split seems to produce these 2 values.\n",
    "\n",
    "    valid_generator=train_datagen.flow_from_dataframe(\n",
    "        dataframe=valid_df,\n",
    "        x_col=\"img_path\",\n",
    "        y_col=\"path\",\n",
    "        batch_size=batch_size,\n",
    "        # shuffle=False,\n",
    "        class_mode=\"binary\",\n",
    "        #classes=lbls,\n",
    "        target_size=(100,100), #gave strange error about tuple cannot be interpreted as integer\n",
    "        subset='validation') #the subset of data from the ImageDataGenerator definition above. The validation_split seems to produce these 2 values.\n",
    "\n",
    "    return train_generator, valid_generator, train_datagen\n",
    "\n",
    "\n",
    "\n",
    "train_generator, valid_generator, train_datagen = create_generators()\n",
    "\n",
    "class_map = {v: k for k, v in train_generator.class_indices.items()}???????"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from keras.applications import resnet50\n",
    "\n",
    "train_generator, valid_generator, train_datagen = create_generators(preprocess_func = resnet50.preprocess_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#the total number of images we have:\n",
    "train_size = len(train_generator.filenames)\n",
    "#train_steps is how many steps per epoch Keras runs the genrator. One step is batch_size*images\n",
    "train_steps = train_size/batch_size\n",
    "#use 2* number of images to get more augmentations in. some do, some dont. up to you\n",
    "train_steps = int(2*train_steps)\n",
    "#same for the validation set\n",
    "valid_size = len(valid_generator.filenames)\n",
    "valid_steps = valid_size/batch_size\n",
    "valid_steps = int(2*valid_steps) "
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nav_menu": {},
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "400px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "545px",
    "left": "0px",
    "right": "1154px",
    "top": "111px",
    "width": "212px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "toc_position": {
   "height": "503px",
   "left": "0px",
   "right": "952.167px",
   "top": "107px",
   "width": "300px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
